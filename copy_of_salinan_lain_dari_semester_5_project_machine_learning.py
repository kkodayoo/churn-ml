# -*- coding: utf-8 -*-
"""Copy of Salinan lain dari Semester 5_Project Machine Learning.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1h-j6nZ6t_j8R0Arg0dS6J_nhKAhPMyFY

# IMPORT MODUL
"""

# Melakukan import modul yang akan digunakan
from google.colab import drive
drive.mount('/content/drive')
import sys
import numpy as np
import pandas as pd
import scipy as sp
from scipy import stats
import matplotlib
import matplotlib.pyplot as plt
import seaborn as sns
import sklearn
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA

"""# EDA"""

# import dataset
data = pd.read_csv("/content/drive/MyDrive/Semester 5/MachineLearning/Projek/Dataset/churndatatelcom.csv")
data.head()

data.shape

len(np.unique(data.customerID))

data.describe()

data.info()

# Mengubah tipe data kolom TotalCharges menjadi numerik
data['TotalCharges'] = pd.to_numeric(data['TotalCharges'], errors='coerce')
data['TotalCharges'] = data['TotalCharges'].fillna(0)
data.info()

# Visualisasi dari distribusi fitur numerik
numerical_features = ['tenure', 'MonthlyCharges', 'TotalCharges']

for feature in numerical_features:
    plt.figure(figsize=(8, 5))
    sns.histplot(data[feature], kde=True)
    plt.title(f'Distribution of {feature}')
    plt.xlabel(feature)
    plt.ylabel('Frequency')
    plt.show()

# Visualisasi dari distribusi fitur kategorik
categorical_features = ['gender', 'SeniorCitizen', 'Partner', 'Dependents', 'PhoneService', 'MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies', 'Contract', 'PaperlessBilling', 'PaymentMethod', 'Churn']

for feature in categorical_features:
    plt.figure(figsize=(10, 6))
    sns.countplot(data=data, x=feature, hue=feature, legend=False, palette='viridis')
    plt.title(f'Distribution of {feature}')
    plt.xlabel(feature)
    plt.ylabel('Count')
    plt.show()

# Visualisasi distribusi kolom target
plt.figure(figsize=(8, 5))
sns.countplot(data=data, x='Churn')
plt.title('Distribution of Churn')
plt.xlabel('Churn')
plt.ylabel('Count')
plt.show()

data.skew(numeric_only=True)

data.kurt(numeric_only=True)

data_for_hist=data.select_dtypes(exclude="bool")
data_for_hist.hist(figsize=(20,20), alpha=0.5,
edgecolor="black", grid=False)

numerical_cols = data.select_dtypes(include=np.number).columns
correlation_matrix = data[numerical_cols].corr()

plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Correlation Matrix of Numerical Features')
plt.show()

"""# Data Preprocessing

## Handling missing values
"""

data.isna().sum()

data.isnull().sum()

data = data.drop('customerID', axis=1)

# Mengganti 'No phone service' dan 'No internet service' dengan 'No' untuk konsistensi
columns_to_harmonize = ['MultipleLines', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies']
for col in columns_to_harmonize:
    data[col] = data[col].replace({'No phone service': 'No', 'No internet service': 'No'})

"""## Encoding categorical features"""

data.head()

# One-Hot Encoding untuk fitur kategorikal
categorical_cols_for_ohe = data.select_dtypes(include='object').columns.tolist()
categorical_cols_for_ohe.remove('Churn') # Exclude target variable

data = pd.get_dummies(data, columns=categorical_cols_for_ohe, drop_first=True)

data.head()

# Mengubah kolom target 'Churn' menjadi numerik (Yes=1, No=0)
data['Churn'] = data['Churn'].map({'Yes': 1, 'No': 0})

# Memisahkan fitur (X) dan target (y)
X = data.drop('Churn', axis=1)
y = data['Churn']

data.head()

# Identifikasi fitur numerik yang akan diperiksa outlier
numerical_features_for_outliers = ['tenure', 'MonthlyCharges', 'TotalCharges']

# Hitung Q1, Q3, dan IQR untuk setiap fitur numerik
for feature in numerical_features_for_outliers:
    Q1 = X[feature].quantile(0.25)
    Q3 = X[feature].quantile(0.75)
    IQR = Q3 - Q1

    # Tentukan batas bawah dan atas untuk outlier
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR

    # Identifikasi outlier
    outliers = X[(X[feature] < lower_bound) | (X[feature] > upper_bound)]
    print(f"\nOutliers detected in '{feature}': {len(outliers)} rows")

    # Hapus outlier dari DataFrame X dan y
    # Menggunakan indeks untuk memastikan konsistensi antara X dan y
    X = X[~((X[feature] < lower_bound) | (X[feature] > upper_bound))]
    y = y[X.index] # Pastikan y juga difilter berdasarkan indeks X yang baru

print(f"\nUkuran X setelah menghapus outlier: {X.shape}")
print(f"Ukuran y setelah menghapus outlier: {y.shape}")

# Penskalaan fitur numerik
# Identifikasi kolom numerik yang akan diskala setelah one-hot encoding
numerical_features_to_scale = ['tenure', 'MonthlyCharges', 'TotalCharges']

scaler = StandardScaler()
X[numerical_features_to_scale] = scaler.fit_transform(X[numerical_features_to_scale])

data.head()

display(X.head())

"""# MODELLING"""

# Membagi data menjadi set pelatihan dan set pengujian
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print(f"Ukuran X_train: {X_train.shape}")
print(f"Ukuran X_test: {X_test.shape}")
print(f"Ukuran y_train: {y_train.shape}")
print(f"Ukuran y_test: {y_test.shape}")

# Import Logistic Regression
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

# Inisialisasi model Logistic Regression
logreg = LogisticRegression(random_state=42, solver='liblinear') # solver='liblinear' is good for small datasets and handles L1/L2 penalties

# Melatih model (fit)
logreg.fit(X_train, y_train)

# Melakukan prediksi pada data testing
y_pred = logreg.predict(X_test)
# Menghitung dan menampilkan akurasi
accuracy = accuracy_score(y_test, y_pred)
print(f"Akurasi model Logistic Regression: {accuracy:.4f}")

# Melakukan prediksi pada data testing
y_pred = logreg.predict(X_test)
# Menghitung dan menampilkan akurasi
accuracy = accuracy_score(y_test, y_pred)
print(f"Akurasi model Logistic Regression: {accuracy:.4f}")

# Import Decision Tree Classifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

# Inisialisasi model Decision Tree
dtc = DecisionTreeClassifier(random_state=42)

# Melatih model
dtc.fit(X_train, y_train)

# Melakukan prediksi pada data testing
y_pred_dt = dtc.predict(X_test)

# Menghitung dan menampilkan akurasi
accuracy_dt = accuracy_score(y_test, y_pred_dt)
print(f"Akurasi model Decision Tree: {accuracy_dt:.4f}")

# Import XGBoost Classifier
import xgboost as xgb
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

# Inisialisasi model XGBoost
xgb_model = xgb.XGBClassifier(random_state=42, eval_metric='logloss')

# Melatih model
xgb_model.fit(X_train, y_train)

# Melakukan prediksi pada data testing
y_pred_xgb = xgb_model.predict(X_test)

# Menghitung dan menampilkan akurasi
accuracy_xgb = accuracy_score(y_test, y_pred_xgb)
print(f"Akurasi model XGBoost: {accuracy_xgb:.4f}")

from sklearn.metrics import roc_auc_score

# Calculate predicted probabilities for the positive class (class 1)
y_pred_proba_logreg = logreg.predict_proba(X_test)[:, 1]

# Hitung ROC-AUC score
roc_auc_logreg = roc_auc_score(y_test, y_pred_proba_logreg)
print(f"ROC-AUC Score model Logistic Regression: {roc_auc_logreg:.4f}")

from sklearn.metrics import classification_report, roc_auc_score

# Classification Report for Decision Tree
report_dt = classification_report(y_test, y_pred_dt, output_dict=True)
precision_dt = report_dt['1']['precision']
recall_dt = report_dt['1']['recall']
f1_dt = report_dt['1']['f1-score']

print(f"\nClassification Report for Decision Tree:\n{classification_report(y_test, y_pred_dt)}")

# Calculate predicted probabilities for the positive class (class 1) for Decision Tree
y_pred_proba_dt = dtc.predict_proba(X_test)[:, 1]

# Hitung ROC-AUC score for Decision Tree
roc_auc_dt = roc_auc_score(y_test, y_pred_proba_dt)
print(f"ROC-AUC Score model Decision Tree: {roc_auc_dt:.4f}")

from sklearn.metrics import classification_report, roc_auc_score

# Classification Report for XGBoost
report_xgb = classification_report(y_test, y_pred_xgb, output_dict=True)
precision_xgb = report_xgb['1']['precision']
recall_xgb = report_xgb['1']['recall']
f1_xgb = report_xgb['1']['f1-score']

print(f"\nClassification Report for XGBoost:\n{classification_report(y_test, y_pred_xgb)}")

# Calculate predicted probabilities for the positive class (class 1) for XGBoost
y_pred_proba_xgb = xgb_model.predict_proba(X_test)[:, 1]

# Hitung ROC-AUC score for XGBoost
roc_auc_xgb = roc_auc_score(y_test, y_pred_proba_xgb)
print(f"ROC-AUC Score model XGBoost: {roc_auc_xgb:.4f}")

from sklearn.metrics import classification_report, roc_auc_score

# Initialize dictionaries to store metrics
metrics = {
    'Model': [],
    'Accuracy': [],
    'Precision (Churn)': [],
    'Recall (Churn)': [],
    'F1-Score (Churn)': [],
    'ROC-AUC': []
}

# --- Calculate Logistic Regression Metrics ---
# The 'accuracy' and 'roc_auc_logreg' are already available from previous cells.
# Generate classification report for Logistic Regression to get precision, recall, f1-score
y_pred_logreg = logreg.predict(X_test)
report_logreg = classification_report(y_test, y_pred_logreg, output_dict=True)
precision_logreg = report_logreg['1']['precision']
recall_logreg = report_logreg['1']['recall']
f1_logreg = report_logreg['1']['f1-score']

# --- Logistic Regression Metrics ---
metrics['Model'].append('Logistic Regression')
metrics['Accuracy'].append(accuracy)
metrics['Precision (Churn)'].append(precision_logreg)
metrics['Recall (Churn)'].append(recall_logreg)
metrics['F1-Score (Churn)'].append(f1_logreg)
metrics['ROC-AUC'].append(roc_auc_logreg)

# --- Decision Tree Metrics --- (accuracy_dt, precision_dt, recall_dt, f1_dt, roc_auc_dt are already defined)
metrics['Model'].append('Decision Tree')
metrics['Accuracy'].append(accuracy_dt)
metrics['Precision (Churn)'].append(precision_dt)
metrics['Recall (Churn)'].append(recall_dt)
metrics['F1-Score (Churn)'].append(f1_dt)
metrics['ROC-AUC'].append(roc_auc_dt)

# --- XGBoost Metrics --- (accuracy_xgb, precision_xgb, recall_xgb, f1_xgb, roc_auc_xgb are already defined)
metrics['Model'].append('XGBoost')
metrics['Accuracy'].append(accuracy_xgb)
metrics['Precision (Churn)'].append(precision_xgb)
metrics['Recall (Churn)'].append(recall_xgb)
metrics['F1-Score (Churn)'].append(f1_xgb)
metrics['ROC-AUC'].append(roc_auc_xgb)

# Create a DataFrame for comparison
performance_df = pd.DataFrame(metrics)
print("\nModel Performance Comparison:")
display(performance_df)

# This cell contains code for saving trained models and the scaler.
# For better notebook organization, this code is typically placed after all model training and evaluation steps.

import pickle

# Save Logistic Regression model
with open('logistic_regression_model.pkl', 'wb') as file:
    pickle.dump(logreg, file)
print("Logistic Regression model saved as 'logistic_regression_model.pkl'")

# Save Decision Tree model
with open('decision_tree_model.pkl', 'wb') as file:
    pickle.dump(dtc, file)
print("Decision Tree model saved as 'decision_tree_model.pkl'")

# Save XGBoost model
with open('xgboost_model.pkl', 'wb') as file:
    pickle.dump(xgb_model, file)
print("XGBoost model saved as 'xgboost_model.pkl'")

# Save the StandardScaler
with open('scaler.pkl', 'wb') as file:
    pickle.dump(scaler, file)
print("StandardScaler saved as 'scaler.pkl'")

# --- Explanation for deployment (added based on user query) ---
print("\nUntuk deployment web, Anda akan membutuhkan:\n")
print("1. 'scaler.pkl': File ini penting untuk menskala data input baru sebelum prediksi, menggunakan penskalaan yang sama saat pelatihan.\n")
print("2. Model terbaik. Berdasarkan evaluasi sebelumnya, 'logistic_regression_model.pkl' adalah model dengan performa terbaik (Akurasi dan ROC-AUC tertinggi).\n")
print("Disarankan untuk menggunakan 'scaler.pkl' bersama dengan 'logistic_regression_model.pkl' untuk prediksi di aplikasi web Anda.")